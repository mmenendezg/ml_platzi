{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: [\"[Errno 2] The file to load file system plugin from does not exist.: '/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so'\"]\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: [\"dlopen(/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io.so, 0x0006): tried: '/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file), '/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io.so' (no such file)\"]\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "import tensorflow as tf\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_net = tf.keras.applications.MobileNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <keras.engine.functional.Functional object at 0x163d1f890>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception_net.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "tags = response.text.split(\"\\n\")\n",
    "\n",
    "\n",
    "def classify_image(input_):\n",
    "    input_ = tf.image.resize(input_, (224, 224))\n",
    "    input_ = tf.reshape(input_, (-1, 224, 224, 3))\n",
    "    input_ = tf.keras.applications.mobilenet_v2.preprocess_input(input_)\n",
    "    prediction = tf.reshape(inception_net(input_), [-1])\n",
    "    confidences = {tags[i]: float(prediction[i]) for i in range(1000)}\n",
    "    return confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=classify_image,\n",
    "    inputs=gr.Image(height=350, width=350, sources=[\"upload\", \"clipboard\"]),\n",
    "    outputs=gr.Label(num_top_classes=5),\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model from: https://huggingface.co/microsoft/swin-tiny-patch4-window7-224\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"Demo Gradio\"\n",
    "description = \"Image classification using Hugging Face\"\n",
    "\n",
    "gr.load(\n",
    "    \"huggingface/microsoft/swin-tiny-patch4-window7-224\",\n",
    "    inputs=gr.Image(label=\"Load the image here!!\"),\n",
    "    title=title,\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53-spanish were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53-spanish and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-large-xlsr-53-spanish\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = model(audio)[\"texto\"]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 833, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/lj/b6nh4d2n3_b608c7t9s86ybc0000gn/T/ipykernel_4224/1129706329.py\", line 4, in transcribe\n",
      "    text = model(audio)[\"texto\"]\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 356, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1132, in __call__\n",
      "    return next(\n",
      "           ^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "                           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 481, in preprocess\n",
      "    raise ValueError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "ValueError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 833, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/lj/b6nh4d2n3_b608c7t9s86ybc0000gn/T/ipykernel_4224/1129706329.py\", line 4, in transcribe\n",
      "    text = model(audio)[\"texto\"]\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 356, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1132, in __call__\n",
      "    return next(\n",
      "           ^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "                           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 481, in preprocess\n",
      "    raise ValueError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "ValueError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.11/site-packages/gradio/queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: None\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=[gr.Audio(sources=[\"microphone\"], type=\"filepath\")],\n",
    "    outputs=[\"textbox\"]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

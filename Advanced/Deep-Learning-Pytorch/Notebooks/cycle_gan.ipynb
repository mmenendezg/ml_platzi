{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join('..', 'Datasets', 'summer2winter_yosemite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../functions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        conv_block = [\n",
    "            # Better padding that zero padding to conserve image's distribution\n",
    "            nn.ReflectionPad2d(1), \n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            # Not as good in normalization as BN, but it is better conserving the contrast\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        ]\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, 64, F),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "        ]\n",
    "\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "\n",
    "        # Encoding\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # Residual transformations\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # Decoding\n",
    "        out_features = in_features / 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_features, out_features, 3, stride=2, padding=1, output_padding=1\n",
    "                ),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        model += [nn.ReflectionPad2d(3), nn.Conv2d(64, output_nc, 7), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discrimator(nn.Module):\n",
    "    \"\"\"This will replicate the PatchGAN. Discriminates style or texture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_nc):\n",
    "        super(Discrimator, self).__init__()\n",
    "        \n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, base_dir, transforms=None, split='train'):\n",
    "        self.transform = transforms.Compose(transforms)\n",
    "        self.files_A = sorted(glob.glob(os.path.join(base_dir, f\"{split}/A/*.*\")))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(base_dir, f\"{split}/B/*.*\")))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_A = self.transform(Image.open(self.files_A[idx]))\n",
    "        image_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B)-1)]))\n",
    "\n",
    "        return {\"A\": image_A, \"B\":image_B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Class():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d7b166ebd68e38de58fc1f6af134f0f2ff66e0d51839b30fdba22526be05163"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "delayed-wholesale",
   "metadata": {},
   "source": [
    "# Obtaining all the articles and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "altered-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WEBSITE = {\n",
    "    'diario_es' : {\n",
    "        'url' : 'https://diarioelsalvador.com/',\n",
    "        'attrs' : {\n",
    "            'class_sections' : 'jeg_menu',\n",
    "            'class_articles' : 'jeg_posts',\n",
    "            'class_article_title' : 'jeg_post_title',\n",
    "            'class_subtitle_article' : 'jeg_post_subtitle',\n",
    "            'class_date_article' : 'jeg_meta_date',\n",
    "            'class_author_article' : 'jeg_meta_author',\n",
    "            'class_body_article' : 'content-inner',            \n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "def main(newspaper):\n",
    "    \"\"\"This method obtains all the articles of each section. \n",
    "    \n",
    "    Args:\n",
    "        newspaper: the id of the newspaper to scrap.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        diario_es = requests.get(WEBSITE[newspaper]['url'])\n",
    "        \n",
    "        if diario_es.status_code == 200:\n",
    "            soup_diario_es = BeautifulSoup(diario_es.text, 'lxml');\n",
    "            sections = _get_sections(soup_diario_es, newspaper)\n",
    "            articles = _get_articles(sections, newspaper)\n",
    "            _save_articles(articles, newspaper)\n",
    "            print(f'The Scrapping of {newspaper}is finished.')\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'Error: Status {diario_es.status_code}')\n",
    "            \n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "        \n",
    "    \n",
    "def _get_sections(soup_newspaper, newspaper):\n",
    "    \"\"\"This method takes the soup of the main page of the website \n",
    "    and returns a list of title and url of each section.\n",
    "    \n",
    "    Args: \n",
    "        soup_newspaper: BeautifulSoup object of the main page of the\n",
    "            website.\n",
    "            \n",
    "        newspaper: the id of the newspaper to scrap.\n",
    "    \n",
    "    Returns:\n",
    "        list_sections: list of the title and url of each section in the \n",
    "            main page of the website.\n",
    "    \"\"\"\n",
    "    print('Getting the sections of the website')\n",
    "    attrs = {'class' : WEBSITE[newspaper]['attrs']['class_sections']}\n",
    "    sections = soup_newspaper.find('ul', attrs=attrs).find_all('li')\n",
    "    \n",
    "    list_sections = []\n",
    "    \n",
    "    if sections:\n",
    "        for section in sections:\n",
    "            url_section = section.find('a').get('href')\n",
    "            title_section = section.find('a').get_text()\n",
    "            section = [title_section, url_section]\n",
    "            list_sections.append(section)\n",
    "    \n",
    "    return list_sections\n",
    "\n",
    "\n",
    "def _get_articles(sections, newspaper):\n",
    "    \"\"\"This method\n",
    "    \n",
    "    Args:\n",
    "        sections: list of the title and url of each section in the \n",
    "            main page of the website.\n",
    "        \n",
    "        newspaper: the id of the newspaper to scrap.\n",
    "    \"\"\"\n",
    "    print('Getting the articles of each section')\n",
    "    attrs = {'class' : WEBSITE[newspaper]['attrs']['class_articles']}\n",
    "    \n",
    "    all_articles =[]\n",
    "    for section in sections:\n",
    "        try:\n",
    "            section_page = requests.get(section[1])\n",
    "            \n",
    "            if section_page.status_code == 200:\n",
    "                soup_section_page = BeautifulSoup(section_page.text, 'lxml')\n",
    "                list_articles = soup_section_page.find('div', attrs=attrs)\n",
    "                if list_articles:\n",
    "                    articles = _get_list_articles(list_articles, section[0])\n",
    "                    all_articles += articles\n",
    "            else:\n",
    "                raise ValueError(f'Error: Status {section_page.status_code} for section {section[0]}')\n",
    "                \n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "def _get_list_articles(list_articles, section_title):\n",
    "    \"\"\"This method returns the link and title for every article\n",
    "    \n",
    "    Args: \n",
    "        list_articles: list of soup of every article.\n",
    "        section_title: title of the section of the article\n",
    "        \n",
    "    Returns:\n",
    "        articles: list of the articles with title, url and section.\n",
    "    \"\"\"\n",
    "    print(f'\\tGetting the list of articles of section {section_title}')\n",
    "    attrs = {'class' : WEBSITE[newspaper]['attrs']['class_article_title']}\n",
    "    \n",
    "    articles = []\n",
    "    for article in list_articles:\n",
    "        url_article = article.find('h3', attrs=attrs).find('a').get('href')\n",
    "        content_article = _get_content_article(url_article, section_title)\n",
    "        articles.append(content_article)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def _get_content_article(url_article, section_title):\n",
    "    \"\"\"This method obtains the information of the article\n",
    "    \n",
    "    Args: \n",
    "        url_article: The url of the article to scrap.\n",
    "        section_title: Name of the section to add to the list\n",
    "    \n",
    "    Returns:\n",
    "        content_article: List that contains all the content of the page\n",
    "            of the article\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = requests.get(url_article)\n",
    "        if article.status_code == 200:\n",
    "            soup_article = BeautifulSoup(article.text, 'lxml')\n",
    "            \n",
    "            title = soup_article.find('h1', attrs={'class' : 'jeg_post_title'})\n",
    "            title_article = [title.get_text() if title else []]\n",
    "                \n",
    "            subtitle = soup_article.find('h2', attrs={'class' : 'jeg_post_subtitle'})\n",
    "            subtitle_article = [subtitle.get_text() if subtitle else []]\n",
    "            \n",
    "            date = soup_article.find('div', attrs={'class' : 'jeg_meta_date'})\n",
    "            date_article = [date.get_text() if date else []]\n",
    "            \n",
    "            author = soup_article.find('div', attrs={'class' : 'jeg_meta_author'})\n",
    "            author_article = [author.get_text() if author else []]\n",
    "            \n",
    "            body = soup_article.find('div', attrs={'class' : 'content-inner'})\n",
    "            body_article = [body.get_text() if body else []]\n",
    "            \n",
    "            content_article = [title_article, subtitle_article, date_article, author_article, body_article, section_title, url_article]\n",
    "            return content_article\n",
    "        else:\n",
    "            raise ValueError(f'Error: Status {article.status_code}')\n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "\n",
    "def _save_articles(articles, newspaper):\n",
    "    \"\"\"This method creates a dataframe with all the information received \n",
    "    and saves the information scraped into a csv file with the name\n",
    "    of the newspaper and the date of the scrapping.\n",
    "    \n",
    "    Args:\n",
    "        articles: List of the information scrapped from the website.\n",
    "        newspaper: name of the website scrapped.\n",
    "    \"\"\"\n",
    "    print('Saving the information.')\n",
    "    today = date.today().strftime('%d-%m-%Y')\n",
    "    filename = f'{newspaper}_{today}.csv'\n",
    "    \n",
    "    columns_articles = [\n",
    "        'title_article',\n",
    "        'subtitle_article',\n",
    "        'date_article',\n",
    "        'author_article',\n",
    "        'body_article',\n",
    "        'section_title',\n",
    "        'url_article'\n",
    "    ]\n",
    "    \n",
    "    df_articles = pd.DataFrame(articles, columns=columns_articles)\n",
    "    df_articles.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "geographic-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the sections of the website\n",
      "Getting the articles of each section\n",
      "\tGetting the list of articles of section DePaís\n",
      "\tGetting the list of articles of section DePalabra\n",
      "\tGetting the list of articles of section DeDinero\n",
      "\tGetting the list of articles of section DeComercio\n",
      "\tGetting the list of articles of section DePlaneta\n",
      "\tGetting the list of articles of section DeInnovación\n",
      "\tGetting the list of articles of section DeCultura\n",
      "\tGetting the list of articles of section DeVida\n",
      "\tGetting the list of articles of section DeDiversión\n",
      "\tGetting the list of articles of section DeDeportes\n",
      "Saving the information.\n",
      "The Scrapping of diario_esis finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    newspaper = 'diario_es'\n",
    "    main(newspaper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

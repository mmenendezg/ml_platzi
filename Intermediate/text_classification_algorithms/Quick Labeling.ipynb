{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punk: Package 'punk' not found in index\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mmenendezg/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/mmenendezg/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     /Users/mmenendezg/nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n",
      "/Users/mmenendezg/Developer/Platzi/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 43.5MB/s]                    \n",
      "2023-02-06 17:20:17 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.4.1/models/default.zip: 100%|██████████| 603M/603M [01:14<00:00, 8.11MB/s] \n",
      "2023-02-06 17:21:37 INFO: Finished downloading models and saved to /Users/mmenendezg/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, UnigramTagger as ut, BigramTagger as bt\n",
    "nltk.download(\"punk\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"tagsets\")\n",
    "\n",
    "# Spanish\n",
    "nltk.download(\"cess_esp\")\n",
    "from nltk.corpus import cess_esp as cess\n",
    "import stanza\n",
    "stanza.download('es')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('here', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('enjoying', 'VBG'),\n",
       " ('with', 'IN'),\n",
       " ('bebe', 'NN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"And now here I am enjoying with bebe\"\n",
    "text = word_tokenize(string)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "None\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "None\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for tag in [\"CC\", \"RB\", \"PRP\", \"VBP\"]:\n",
    "    print(nltk.help.upenn_tagset(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('permit', 'VB'),\n",
       " ('other', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('residence', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"They do not permit other people to get residence permit\"\n",
    "text = word_tokenize(string)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/jxzw14n53ksgt9_q98rq0_0m0000gn/T/ipykernel_29287/3570717361.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  uni_tagger.evaluate(cess_sents[fraction:])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8068832283915284"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cess_sents = cess.tagged_sents()\n",
    "fraction = int(len(cess_sents) * 0.9)\n",
    "uni_tagger = ut(cess_sents[:fraction])\n",
    "uni_tagger.evaluate(cess_sents[fraction:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Estoy', 'vmip1s0'),\n",
       " ('con', 'sps00'),\n",
       " ('mi', 'dp1css'),\n",
       " ('bebe', None),\n",
       " ('aqui', None),\n",
       " ('cocinando', 'vmg0000')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_tagger.tag(\"Estoy con mi bebe aqui cocinando\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bs/jxzw14n53ksgt9_q98rq0_0m0000gn/T/ipykernel_29287/1141694831.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  bi_tagger.evaluate(cess_sents[fraction:])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10983113909559244"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_tagger = bt(cess_sents[:fraction])\n",
    "bi_tagger.evaluate(cess_sents[fraction:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Estoy', None),\n",
       " ('con', None),\n",
       " ('mi', None),\n",
       " ('bebe', None),\n",
       " ('aqui', None),\n",
       " ('cocinando', None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_tagger.tag(\"Estoy con mi bebe aqui cocinando\".split(\" \"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 17:29:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 12.7MB/s]                    \n",
      "2023-02-06 17:29:21 WARNING: Language es package default expects mwt, which has been added\n",
      "2023-02-06 17:29:22 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "=======================\n",
      "\n",
      "2023-02-06 17:29:22 INFO: Use device: cpu\n",
      "2023-02-06 17:29:22 INFO: Loading: tokenize\n",
      "2023-02-06 17:29:22 INFO: Loading: mwt\n",
      "2023-02-06 17:29:22 INFO: Loading: pos\n",
      "2023-02-06 17:29:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(\"es\", processors=\"tokenize, pos\")\n",
    "doc = nlp(\"Estoy aprendiendo a procesar lenguaje natural\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy AUX\n",
      "aprendiendo VERB\n",
      "a ADP\n",
      "procesar VERB\n",
      "lenguaje NOUN\n",
      "natural ADJ\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d7b166ebd68e38de58fc1f6af134f0f2ff66e0d51839b30fdba22526be05163"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     /Users/mmenendezg/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6030\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.corpus.cess_esp.sents()\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n"
     ]
    }
   ],
   "source": [
    "flatten = [w for l in corpus for w in l]\n",
    "print((flatten[:20]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of method `re.search()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['millones', 'dólares', 'es', 'alcaldesa', 'Congreso', 'esta', 'militantes', 'esta', 'eso', 'es']\n"
     ]
    }
   ],
   "source": [
    "# Contains `es` in any point\n",
    "arr = [w for w in flatten if re.search(\"es\", w)]\n",
    "print(arr[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'militantes', 'populares', 'militantes', 'elecciones', 'generales', 'elecciones', 'generales', 'factores', 'alcaldes']\n"
     ]
    }
   ],
   "source": [
    "# Contains `es` at the end of the word\n",
    "arr = [w for w in flatten if re.search(\"es$\", w)]\n",
    "print(arr[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'es', 'este', 'españolas', 'esta', 'este', 'es', 'español', 'es', 'español']\n"
     ]
    }
   ],
   "source": [
    "# Contains `es` at the beginning of the word\n",
    "arr = [w for w in flatten if re.search(\"^es\", w)]\n",
    "print(arr[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hotel', 'gracias', 'hoy', 'generales', 'hombre', 'ha', 'hasta', 'gracias', 'gente', 'ha']\n"
     ]
    }
   ],
   "source": [
    "# Range [a-z][ghi]\n",
    "arr = [w for w in flatten if re.search(\"^[ghi]\", w)]\n",
    "print(arr[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_obstante', 'no', 'no', 'no', 'norte', 'no', 'no', 'no', 'no', 'norteamericano']\n"
     ]
    }
   ],
   "source": [
    "# Quantifiers\n",
    "# * -> 0 or more\n",
    "# + -> 1 or more \n",
    "# ? -> 0 or 1\n",
    "arr = [w for w in flatten if re.search(\"^(no)+\", w)]\n",
    "print(arr[10:20])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is \n",
      " a test\n"
     ]
    }
   ],
   "source": [
    "text = \"This is \\n a test\"\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization: It's the process of subdividing a text chain in minimum linguistic units (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = \"\"\" Cuando sea el rey del mundo (imaginaba él en su cabeza) no tendré que preocuparme por estas bobadas. \n",
    "                Era solo un niño de 7 años, pero pensaba que podría ser cualquier cosa que su imaginación le permitiera\n",
    "                visualizar en su cabeza ...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '(imaginaba', 'él', 'en', 'su', 'cabeza)', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# Case 1: tokenize using the spaces\n",
    "tokenized_text_1 = re.split(r' ', text_test)\n",
    "print(tokenized_text_1[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', '(imaginaba', 'él', 'en', 'su', 'cabeza)', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas.', 'Era']\n"
     ]
    }
   ],
   "source": [
    "# Case 2: tokenize using Regex\n",
    "tokenized_text_2 = re.split(r'[ \\t\\n]+', text_test)\n",
    "print(tokenized_text_2[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', 'imaginaba', 'él', 'en', 'su', 'cabeza', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas', 'Era']\n"
     ]
    }
   ],
   "source": [
    "# Case 3: \n",
    "tokenized_text_2 = re.split(r'[ \\W\\t\\n]+', text_test)\n",
    "print(tokenized_text_2[:20])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En', 'los', 'E', 'U', 'esa', 'postal', 'vale', '15', '50', '']\n"
     ]
    }
   ],
   "source": [
    "text_sample_2 = \"En los E.U. esa postal vale $15.50 ...\"\n",
    "tokenized_text = re.split(r'[ \\W\\t\\n]+', text_sample_2)\n",
    "print(tokenized_text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"\"\"(?x)                  # set flag to allow verbose regexps\n",
    "              (?:[A-Z]\\.)+          # abbreviations, e.g. U.S.A\n",
    "              | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "              | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages\n",
    "              | \\.\\.\\.              # ellipsis\n",
    "              | [][.,:\"'?():-_`]    # these are separate tokens; includes [,]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En', 'los', 'E.U.', 'esa', 'postal', 'vale', '$15.50', '...']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.regexp_tokenize(text_sample_2, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d7b166ebd68e38de58fc1f6af134f0f2ff66e0d51839b30fdba22526be05163"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

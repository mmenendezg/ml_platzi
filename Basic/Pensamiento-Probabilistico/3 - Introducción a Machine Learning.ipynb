{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Introducción a Machine Learning\n",
    "\n",
    "## Feature Vectors\n",
    "****\n",
    "* Se utilizan para representar características simbólicas o numéricas llamadas features. Ejemmplo de feature vector es la represetación de RGB :\n",
    ">```color = [R, G, B]```\n",
    "\n",
    "* Permiten analizar un objeto desde una perspectiva matemática. \n",
    "\n",
    "* Los algoritmos de Machine Learning usan valores numéricos como input. \n",
    "\n",
    "## Métricas de Distancia\n",
    "****\n",
    "* Muchos de los algoritmos de Machine Learning se pueden clasificar como algoritmos de optimización. \n",
    "\n",
    "* Lo que se se desea optimizar es una función que determina la distancia entre features.\n",
    "\n",
    ">$x = (a, b)$\n",
    ">$y = (c, d)$\n",
    ">\n",
    "> Distancia Euclidiana ==> $\\sqrt{(a - c)^2 + (b - d)^2}$\n",
    ">\n",
    "> Distancia de Manhattan ==> $\\mid a - c\\mid + \\mid b - d\\mid$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introducción al agrupamiento (Clustering)\n",
    "****\n",
    "* Es un algoritmo que permite agrupar datos con sus similares en clusters. Permite a su vez evluar la estructura de los datos y la similitud entre valores dentro del mismo cluster\n",
    "\n",
    "* Es parte de algoritmos de aprendizaje no supervisado, ya que no requiere _labels_\n",
    "\n",
    "### Agrupamiento jerárquico\n",
    "\n",
    "* Agrupa objetos similares en clusters\n",
    "\n",
    "* El algoritmo trata a cada objeto como un cluster individual, y luego realiza lo siguiente recursivamente:\n",
    "> - Identifica los dos clusters con menor distancia. \n",
    ">\n",
    "> - Agrupa los dos clusters mas cercanos\n",
    "\n",
    "* El output final es el dendograma, que muestra la relación entre objetos y grupos. \n",
    "\n",
    "* Es importante determinar que métrica de distancia se usará y la cantidad de objetos en cada cluster (linkage criteria)\n",
    "\n",
    "### K-means\n",
    "\n",
    "* Es un algoritmo que agrupa usando centroides\n",
    "\n",
    "* El algoritmo asigna puntos al azar (K define la cantidad inicial de clusters) y luego:\n",
    "> - En cada iteración el punto se ajusta a su nuevo centroide y cada punto se recalcula con la distancia respecto a los centroides. \n",
    ">\n",
    "> - Los puntos se reasignan al nuevo centro \n",
    ">\n",
    "> - El algoritmo se repite iterativamente hasta que no existan mas mejoras posibles \n",
    "\n",
    "### Tipos de agrupamiento\n",
    "\n",
    "> **Agrupamiento estricto** : un valor solo puede pertenecer a un grupo, no puede pertenecer a multiples grupos. \n",
    ">\n",
    "> **Agrupamiento laxo** : A cada dato se le asignan probabilidades de pertenecer o no a un grupo. \n",
    "\n",
    "### Modelos para determinar similitudes\n",
    "\n",
    "> **Modelos Conectivos** : Los puntos mas similares son los puntos que se encuentran cercanos entre sí en el espacio de busqueda. El espacio de busqueda está determinado por las dimensiones del *Feature Vector*.\n",
    ">\n",
    "> **Modelos de Centroide** : Su similitud es proporcional a la distancia con el centroide del grupo. \n",
    ">\n",
    "> **Modelos de distribución** : Determina mediante probabilidades asignadas si pertenecen o no a una distribución específica. \n",
    ">\n",
    "> **Modelos de Densidad** : Analizan la densidad de los datos en diferentes regiones y dividen el conjunto en grupos, luego asignan los puntos de acuerdo a las áreas de densidad en las que se haya dividido el dataset. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}